# PrevisÃ£o do Tempo - Open-Meteo ğŸŒ¤ï¸

**Arquitetura End-to-End:** Python/Docker â†’ S3 â†’ Databricks Lakehouse (Medalion)

Sistema completo para coleta automatizada de dados climÃ¡ticos, processamento em pipeline e transformaÃ§Ã£o em **Lakehouse** usando Databricks.

## ğŸ“Œ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       INGESTÃƒO (Python/Docker)                      â”‚
â”‚  - Open-Meteo API (dados D-1)                                       â”‚
â”‚  - Coleta diÃ¡ria/horÃ¡ria para todos os municÃ­pios BR                â”‚
â”‚  - Salva CSV localmente ou Parquet em S3                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   AWS S3     â”‚
                    â”‚ /raw/clima/  â”‚ â† Dados brutos (Parquet)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   DATABRICKS LAKEHOUSE (Medalion)       â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚  Bronze: Raw data                  â”‚ â”‚
         â”‚  â”‚  (cloud_files + SLT)               â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚               â”‚                         â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚  Silver: Transformed & Clean       â”‚ â”‚
         â”‚  â”‚  (dedupe, tipos, normalizaÃ§Ã£o)     â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚               â”‚                         â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚  â”‚  Gold: Metrics & Analytics         â”‚ â”‚
         â”‚  â”‚  (agregaÃ§Ãµes, KPIs)                â”‚ â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Sobre o Projeto

Sistema Python que coleta dados meteorolÃ³gicos histÃ³ricos (D-1) de forma robusta e eficiente, com suporte a:
- **Dados DiÃ¡rios**: Temperatura mÃ¡x/mÃ­n, sensaÃ§Ã£o tÃ©rmica, precipitaÃ§Ã£o, neve, vento, radiaÃ§Ã£o solar
- **Dados HorÃ¡rios**: Temperatura, umidade relativa, precipitaÃ§Ã£o, velocidade do vento
- **Multi-municÃ­pio**: Processa simultaneamente todos os municÃ­pios brasileiros
- **Retry automÃ¡tico**: Tratamento inteligente de falhas de conexÃ£o
- **Upload S3**: Salva dados em Parquet com particionamento Hive-style
- **Databricks DLT**: Pipelines de transformaÃ§Ã£o automatizados (Bronze â†’ Silver â†’ Gold)

## ğŸš€ Quick Start

### âš ï¸ IMPORTANTE: ConfiguraÃ§Ã£o de Credenciais

**NÃƒO commite o arquivo `.env` com credenciais reais!**

Este projeto usa `.env` para armazenar credenciais AWS. Para seguranÃ§a:

1. Copie `.env.example` para `.env` (local, nÃ£o versionado)
2. Preencha com suas credenciais reais
3. `.gitignore` garante que `.env` nunca serÃ¡ commitado

```bash
cp .env.example .env
# Edite .env com suas credenciais reais
```

### PrÃ©-requisitos

- **Python 3.8+**
- **pip** ou **conda**
- **Docker** (para rodar containerizado)
- **AWS Credentials** (para upload S3)
- **Databricks Account** (para pipeline de transformaÃ§Ã£o)

### InstalaÃ§Ã£o Local

#### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/guigeo/previsao-do-tempo-open-meteo.git
cd previsao-do-tempo-open-meteo
```

#### 2. Crie e ative um ambiente virtual

macOS / Linux (zsh/bash):
```bash
python -m venv .venv
source .venv/bin/activate
```

Windows (PowerShell):
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

#### 3. Instale dependÃªncias

```bash
pip install -r requirements.txt
```

#### 4. Configure credenciais AWS (opcional para upload S3)

```bash
# Configure um profile AWS (ex: "open-meteo")
aws configure --profile open-meteo
# ForneÃ§a Access Key, Secret Key, region, output
```

Ou edite `.env` diretamente:
```bash
AWS_ACCESS_KEY_ID=sua-chave
AWS_SECRET_ACCESS_KEY=sua-secret
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET=seu-bucket-s3
```

#### 5. Verifique a lista de municÃ­pios

```bash
# Arquivo deve existir e conter: codigo_ibge, nome, nome_uf, latitude, longitude
head data/lista_municipios/lista_mun.csv
```

## Passo a passo â€” executar o projeto

### ExecuÃ§Ã£o Local (Python)

#### 1. Coleta diÃ¡ria/horÃ¡ria

```bash
# Coleta modo padrÃ£o (ambos: diÃ¡rio + horÃ¡rio)
python main.py

# Apenas diÃ¡rios
python main.py --modo diario

# Apenas horÃ¡rios
python main.py --modo horario
```

Os arquivos gerados ficam em `data/raw/` com nomes padronizados:
- `dados_climaticos_diarios_YYYYMMDD.csv`
- `dados_climaticos_horarios_YYYYMMDD.csv`

#### 2. Backfill histÃ³rico + upload S3

```bash
python scripts/backfill_once.py
```

Este script:
- Faz fetch histÃ³rico (endpoint `archive` da Open-Meteo)
- Salva em Parquet (compressÃ£o snappy) em `data/raw/diario/` e `data/raw/horario/`
- Realiza upload para S3 automaticamente

**Edite o topo do script para alterar:**
- `DATA_INI` / `DATA_FIM` (intervalo de datas)
- `SLEEP_BETWEEN_CALLS` (para throttling)
- `BUCKET` / `PROFILE` (destino S3)

### ExecuÃ§Ã£o via Docker

#### 1. Build da imagem

```bash
docker-compose build --no-cache
```

#### 2. Executar container

```bash
# Executa com comando padrÃ£o (python main.py --modo ambos)
docker-compose up --abort-on-container-exit

# Rodar em background
docker-compose up -d
docker-compose logs -f
```

#### 3. Customizar comando

Edite `docker-compose.yml` e altere o `command`:
```yaml
services:
  openmeteo:
    # ...
    command: ["python", "main.py", "--modo", "diario"]
```

Ou sobrescreva na CLI:
```bash
docker-compose run --rm openmeteo python main.py --modo horario
```

#### 4. VariÃ¡veis de ambiente

O `docker-compose.yml` lÃª do arquivo `.env`. Certifique-se que contÃ©m:
```
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET=seu-bucket
```

### Pipeline Databricks (TransformaÃ§Ã£o)

Para configurar e executar o pipeline de transformaÃ§Ã£o no Databricks, consulte **[databricks/README.md](./databricks/README.md)**.

Resumo:
1. Crie um catÃ¡logo `open_meteo` com schemas `bronze`, `silver`, `gold`
2. Configure acesso ao S3 via IAM role ou secrets
3. Crie um pipeline DLT apontando para os scripts SQL em `databricks/pipeline_dlt/`
4. Execute: os dados do S3 serÃ£o ingeridos e transformados automaticamente

## ğŸ“Š Estrutura de Dados

### Dados DiÃ¡rios
| Campo | DescriÃ§Ã£o |
|-------|-----------|
| `data` | Data (formato YYYY-MM-DD) |
| `temp_max_c` | Temperatura mÃ¡xima (Â°C) |
| `temp_min_c` | Temperatura mÃ­nima (Â°C) |
| `sensacao_termica_max_c` | SensaÃ§Ã£o tÃ©rmica mÃ¡xima (Â°C) |
| `sensacao_termica_min_c` | SensaÃ§Ã£o tÃ©rmica mÃ­nima (Â°C) |
| `precipitacao_total_mm` | PrecipitaÃ§Ã£o total (mm) |
| `chuva_mm` | Chuva (mm) |
| `neve_mm` | Neve (mm) |
| `vento_velocidade_max_kmh` | Velocidade mÃ¡x do vento (km/h) |
| `rajadas_vento_max_kmh` | Rajadas mÃ¡x do vento (km/h) |
| `vento_direcao_dominante_graus` | DireÃ§Ã£o dominante do vento (Â°) |
| `radiacao_solar_mj_m2` | RadiaÃ§Ã£o solar (MJ/mÂ²) |
| `codigo_tempo_wmo` | CÃ³digo WMO do tempo |
| `municipio`, `uf`, `latitude`, `longitude` | Dados do municÃ­pio |

### Dados HorÃ¡rios
| Campo | DescriÃ§Ã£o |
|-------|-----------|
| `data_hora` | Data/hora (ISO 8601) |
| `temperatura_c` | Temperatura (Â°C) |
| `umidade_relativa` | Umidade relativa (%) |
| `precipitacao_mm` | PrecipitaÃ§Ã£o (mm) |
| `velocidade_vento_ms` | Velocidade do vento (m/s) |
| `municipio`, `uf`, `latitude`, `longitude` | Dados do municÃ­pio |

## ğŸ—‚ï¸ Estrutura do Projeto

```
previsao-do-tempo-open-meteo/
â”œâ”€â”€ .env.example                     # Template de credenciais (versionado)
â”œâ”€â”€ .gitignore                       # Exclui .env, credenciais, dados
â”œâ”€â”€ Dockerfile                       # Imagem Docker
â”œâ”€â”€ docker-compose.yml               # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ README.md                        # Este arquivo
â”œâ”€â”€ requirements.txt                 # DependÃªncias Python
â”œâ”€â”€ main.py                          # Script principal com CLI
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ recupera_dados_api_dia.py   # Coleta dados diÃ¡rios
â”‚   â”œâ”€â”€ recupera_dados_api_hora.py  # Coleta dados horÃ¡rios
â”‚   â”œâ”€â”€ processa_dados.py           # Processamento e traduÃ§Ã£o
â”‚   â””â”€â”€ upload_s3.py                # UtilitÃ¡rio de upload S3 (boto3)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ backfill_once.py            # Backfill histÃ³rico + upload S3
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ README.md                   # Guia de pipeline Databricks
â”‚   â””â”€â”€ pipeline_dlt/
â”‚       â”œâ”€â”€ open_meteo_s3_to_bronze/
â”‚       â”‚   â””â”€â”€ transformations/
â”‚       â”‚       â”œâ”€â”€ get_s3_to_bronze_dia.sql
â”‚       â”‚       â””â”€â”€ get_s3_to_bronze_hora.sql
â”‚       â”œâ”€â”€ open_meteo_bronze_to_silver/
â”‚       â”‚   â””â”€â”€ transformations/
â”‚       â”‚       â”œâ”€â”€ get_bronze_to_silver_dia.sql
â”‚       â”‚       â””â”€â”€ get_bronze_to_silver_hora.sql
â”‚       â””â”€â”€ open_meteo_silver_to_gold/
â”‚           â””â”€â”€ transformations/
â”‚               â””â”€â”€ gold_metricas_clima.sql
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ lista_municipios/
    â”‚   â””â”€â”€ lista_mun.csv           # MunicÃ­pios com coordenadas
    â””â”€â”€ raw/
        â”œâ”€â”€ diario/                 # Dados diÃ¡rios (local)
        â””â”€â”€ horario/                # Dados horÃ¡rios (local)
```

## ğŸ› ï¸ DependÃªncias

- **pandas**: Processamento de dados
- **requests**: RequisiÃ§Ãµes HTTP Ã  API
- **python-dateutil**: ManipulaÃ§Ã£o de datas/timezones
- **tqdm**: Barra de progresso
- **pytz**: Suporte a timezones
- **boto3**: Cliente AWS S3

## âš™ï¸ ConfiguraÃ§Ã£o

As configuraÃ§Ãµes principais estÃ£o em `main.py`:

```python
MODO_COLETA_DEFAULT = "ambos"  # "diario" | "horario" | "ambos"
TIMEZONE = "America/Sao_Paulo"
```

Para backfill, edite o topo de `scripts/backfill_once.py`:

```python
DATA_INI = date(2025, 11, 6)
DATA_FIM = date(2025, 11, 11)
BUCKET = BUCKET
```

## ğŸ”„ Recursos

- âœ… Coleta D-1 (dados do dia anterior)
- âœ… Retry automÃ¡tico com backoff exponencial
- âœ… Suporte a mÃºltiplos timezones
- âœ… Tratamento robusto de erros
- âœ… Progresso visual (tqdm)
- âœ… Encoding UTF-8 com BOM para Excel
- âœ… Upload S3 com particionamento Hive-style
- âœ… Databricks DLT (Bronze â†’ Silver â†’ Gold)
- âœ… Docker + Docker Compose

## ğŸ“ Notas

- A API Open-Meteo Ã© **gratuita** e nÃ£o requer autenticaÃ§Ã£o
- Coleta sempre o D-1 (dia anterior) considerando timezone de SÃ£o Paulo
- Em caso de falha na API, hÃ¡ retry automÃ¡tico (mÃ¡x. 2 tentativas)
- Dados horÃ¡rios possuem fallback entre archive e forecast APIs
- **NUNCA commite o arquivo `.env` com credenciais reais**
- Use `.env.example` como referÃªncia para novos contribuidores

## ğŸ”’ SeguranÃ§a

### Credenciais

- `.env` estÃ¡ no `.gitignore` e nunca serÃ¡ versionado
- Use `.env.example` como template â€” **sempre mantenha atualizado com novas variÃ¡veis**
- Para CI/CD, configure secrets no GitHub Actions ou similar

### IAM Permissions (AWS)

Recomenda-se criar um usuÃ¡rio IAM com permissÃµes mÃ­nimas:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::seu-bucket",
        "arn:aws:s3:::seu-bucket/*"
      ]
    }
  ]
}
```

## ğŸ› Troubleshooting

### "Arquivo nÃ£o encontrado: data/lista_municipios/lista_mun.csv"

- Certifique-se que o arquivo existe
- Verifique o caminho e encoding (UTF-8 com BOM recomendado)

### "Erro de conexÃ£o com S3"

- Confirme credenciais em `.env`
- Teste: `aws s3 ls s3://seu-bucket --profile open-meteo`
- Verifique permissÃµes IAM

### Docker nÃ£o encontra `.env`

- Certifique-se que `.env` estÃ¡ na raiz do projeto
- Run: `docker-compose config` para validar

## ğŸ“š ReferÃªncias

- [Open-Meteo API](https://open-meteo.com/)
- [Databricks Delta Live Tables](https://docs.databricks.com/workflows/delta-live-tables/)
- [Medalion Architecture](https://www.databricks.com/blog/2022/06/24/etl-patterns-at-scale-with-medallion-architecture-and-databricks.html)
- [AWS S3 + IAM](https://docs.aws.amazon.com/s3/)

## ğŸ“„ LicenÃ§a

MIT

## ğŸ‘¤ Autor

guigeo

---

**Ãšltima atualizaÃ§Ã£o**: Novembro de 2025
