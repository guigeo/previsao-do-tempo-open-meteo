# Pipeline Databricks ‚Äî Medalion Architecture üèóÔ∏è

## üìå Vis√£o Geral

Este diret√≥rio cont√©m os pipelines **Delta Live Tables (DLT)** que processam dados clim√°ticos seguindo a arquitetura **Medalion** (Bronze ‚Üí Silver ‚Üí Gold). Os dados s√£o ingeridos do **S3** (enviados pela aplica√ß√£o Python/Docker) e transformados em tabelas otimizadas no **Databricks Lakehouse**.

### Fluxo de Dados

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Raw (Parquet)   ‚îÇ  ‚Üê Python/Docker escreve aqui
‚îÇ /raw/clima/         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BRONZE LAYER (open_meteo_s3_to_bronze)        ‚îÇ
‚îÇ  - Leitura via cloud_files()                    ‚îÇ
‚îÇ  - Schema simples (tudo strings)                ‚îÇ
‚îÇ  - Streaming Live Tables (SLT)                  ‚îÇ
‚îÇ  - Tabelas: clima_diario, clima_horario         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SILVER LAYER (open_meteo_bronze_to_silver)    ‚îÇ
‚îÇ  - Transforma√ß√£o de tipos                       ‚îÇ
‚îÇ  - L√≥gica de neg√≥cio                            ‚îÇ
‚îÇ  - Deduplica, normaliza, enriquece              ‚îÇ
‚îÇ  - Tabelas: clima_diario, clima_horario         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GOLD LAYER (open_meteo_silver_to_gold)        ‚îÇ
‚îÇ  - Agrega√ß√µes, m√©tricas                         ‚îÇ
‚îÇ  - Otimizado para analytics/BI                  ‚îÇ
‚îÇ  - Tabelas: metricas_clima                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üóÇÔ∏è Estrutura de Diret√≥rios

```
databricks/
‚îú‚îÄ‚îÄ README.md                              # Este arquivo
‚îú‚îÄ‚îÄ pipeline_dlt/
‚îÇ   ‚îú‚îÄ‚îÄ open_meteo_s3_to_bronze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformations/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ get_s3_to_bronze_dia.sql  # Bronze ‚Äî dados di√°rios
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ get_s3_to_bronze_hora.sql # Bronze ‚Äî dados hor√°rios
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ open_meteo_bronze_to_silver/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformations/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ get_bronze_to_silver_dia.sql
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ get_bronze_to_silver_hora.sql
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ open_meteo_silver_to_gold/
‚îÇ       ‚îî‚îÄ‚îÄ transformations/
‚îÇ           ‚îî‚îÄ‚îÄ gold_metricas_clima.sql
‚îÇ
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ dlt_config.json (n√£o versioned ‚Äî configura√ß√£o local)
```

## üöÄ Setup no Databricks

### Pr√©-requisitos

1. **Workspace Databricks** ativo (Premium ou acima para DLT)
2. **Cluster** com DBR 13.3+ (runtime com suporte a DLT)
3. **Permiss√µes** para criar cat√°logo e schemas
4. **Acesso ao S3** via IAM role ou credenciais

### Passos de Configura√ß√£o

#### 1. Crie um Cat√°logo e Schema

```sql
-- No Databricks SQL Editor:
CREATE CATALOG IF NOT EXISTS open_meteo;
CREATE SCHEMA IF NOT EXISTS open_meteo.bronze;
CREATE SCHEMA IF NOT EXISTS open_meteo.silver;
CREATE SCHEMA IF NOT EXISTS open_meteo.gold;

-- Defina permiss√µes se necess√°rio
GRANT ALL PRIVILEGES ON CATALOG open_meteo TO `seu-grupo@empresa.com`;
```

#### 2. Configure Acesso ao S3

Se usando IAM role:
```python
# No Databricks Python REPL:
# Sua workspace j√° tem uma IAM role anexada
# Verifique na console AWS que o bucket S3 est√° acess√≠vel
```

Alternativa com credenciais:
```python
# Configure em Databricks Admin > Secrets
spark.conf.set("fs.s3a.access.key", dbutils.secrets.get("scope=aws", "access_key"))
spark.conf.set("fs.s3a.secret.key", dbutils.secrets.get("scope=aws", "secret_key"))
```

#### 3. Crie um DLT Pipeline

No Databricks Workspace:
- V√° para **Workflows** ‚Üí **Delta Live Tables** ‚Üí **Create Pipeline**
- Preecha as informa√ß√µes:
  - **Pipeline name:** `open-meteo-medalion`
  - **Notebook or SQL path:** `/Users/seu-email@empresa.com/open_meteo_s3_to_bronze` (ou similar, depende de onde voc√™ salvou)
  - **Target catalog:** `open_meteo`
  - **Target schema:** `bronze`
  - **Cluster:** Selecione um cluster DLT ou auto-scaling
  - **Modo:** `Development` (para testes) ou `Continuous` (para produ√ß√£o)

#### 4. Configure Vari√°veis de Pipeline

Adicione as seguintes vari√°veis na aba **Advanced** do pipeline:

```json
{
  "input_path_diario": "s3://seu-bucket/raw/clima/diario/",
  "input_path_horario": "s3://seu-bucket/raw/clima/horario/",
  "environment": "dev"
}
```

#### 5. Execute o Pipeline

Clique em **Start** para rodar o pipeline. Os dados do S3 ser√£o lidos e as tabelas ser√£o criadas/atualizadas.

## üìä Estrutura de Dados por Camada

### Bronze (Raw)

**Tabela:** `open_meteo.bronze.clima_diario`

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| data | STRING | Data do registro (YYYY-MM-DD) |
| codigo_ibge | STRING | C√≥digo IBGE do munic√≠pio |
| municipio | STRING | Nome do munic√≠pio |
| uf | STRING | Unidade federativa |
| latitude | STRING | Latitude (coordenada) |
| longitude | STRING | Longitude (coordenada) |
| temp_max_c | STRING | Temperatura m√°xima (¬∞C) |
| temp_min_c | STRING | Temperatura m√≠nima (¬∞C) |
| ... | STRING | Outros campos meteorol√≥gicos |
| ingested_at | TIMESTAMP | Data/hora de ingest√£o |

### Silver (Transformado)

Tipos corrigidos (DOUBLE para temp, INT para c√≥digo_ibge, etc.), l√≥gica de neg√≥cio aplicada, deduplica√ß√£o.

### Gold (Analytics)

Tabelas agregadas e m√©tricas prontas para BI/Analytics.

## üõ†Ô∏è Modificar Transforma√ß√µes

Para ajustar l√≥gica das transforma√ß√µes:

1. Edite os arquivos `.sql` nos respectivos diret√≥rios
2. O DLT atualiza automaticamente em cada execu√ß√£o (modo `Continuous`) ou clique em **Start**
3. Para testes, execute SQL direto no Databricks Editor:

```sql
-- Teste uma transforma√ß√£o individualmente
CREATE OR REPLACE TABLE open_meteo.bronze.clima_diario_test AS
SELECT * FROM cloud_files(
  "s3://seu-bucket/raw/clima/diario/",
  "parquet"
);

SELECT * FROM open_meteo.bronze.clima_diario_test LIMIT 10;
```

## üîó Integra√ß√£o com Python

O Python Docker envia Parquets para S3. O pipeline DLT detecta automaticamente novos arquivos e processa.

**Prefixo S3 esperado:**
```
s3://gbrj-open-meteo-datalake/raw/clima/diario/date=2025-11-21/dados_climaticos_diarios_20251121.parquet
s3://gbrj-open-meteo-datalake/raw/clima/horario/date=2025-11-21/dados_climaticos_horarios_20251121.parquet
```

## üìã Checklista de Deployment

- [ ] Catalog e schemas criados
- [ ] Acesso ao S3 testado
- [ ] Pipeline DLT criado
- [ ] Vari√°veis de pipeline configuradas
- [ ] Pipeline executado com sucesso
- [ ] Dados presentes em `open_meteo.bronze.*`
- [ ] Transforma√ß√µes Silver/Gold executadas
- [ ] Permiss√µes de acesso validadas

## üêõ Troubleshooting

### "Path does not exist" no cloud_files()

**Causa:** S3 path incorreto ou sem dados

**Solu√ß√£o:**
- Confirme o bucket e prefix em `input_path_diario` / `input_path_horario`
- Verifique no console AWS se h√° objetos Parquet nesse path
- Confirme credenciais de acesso

### "Insufficient permissions"

**Causa:** IAM role sem permiss√µes S3

**Solu√ß√£o:**
- Adicione policy S3 √† role Databricks:
  ```json
  {
    "Action": ["s3:GetObject", "s3:ListBucket", "s3:GetObjectVersion"],
    "Effect": "Allow",
    "Resource": ["arn:aws:s3:::seu-bucket", "arn:aws:s3:::seu-bucket/*"]
  }
  ```

### Pipeline n√£o atualiza

**Causa:** Modo `Development` ou cluster n√£o rodando

**Solu√ß√£o:**
- Certifique-se que o cluster est√° ativo
- Para produ√ß√£o, altere para modo `Continuous`
- Clique em **Start** novamente

## üìö Refer√™ncias

- [Databricks Delta Live Tables](https://docs.databricks.com/workflows/delta-live-tables/)
- [Medalion Architecture](https://www.databricks.com/blog/2022/06/24/etl-patterns-at-scale-with-medallion-architecture-and-databricks.html)
- [Cloud Files (Auto Loader)](https://docs.databricks.com/en/ingestion/cloud-object-storage/index.html)

## üë§ Suporte

Para d√∫vidas sobre os pipelines Databricks, consulte o time de Data Engineering.
